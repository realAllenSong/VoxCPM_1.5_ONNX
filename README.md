# VoxCPM 1.5B ONNX (CPU)

ONNX_Lab è‡´åŠ›äºæ‰“é€ ç®€å•æ˜“ç”¨çš„å¼ºå¤§å¼€æº TTS æ¨¡å‹çš„ ONNX CPU è¿è¡Œç‰ˆï¼Œæ—¨åœ¨ä»¥æœ€å°æˆæœ¬è·‘å‡ºæœ€é«˜è´¨é‡è¯­éŸ³ã€‚
ç›®å‰æ”¯æŒ **VoxCPM 1.5B** ï¼ˆ0.5Bç‰ˆæœ¬æš‚æ—¶ä¸æ”¯æŒï¼‰ï¼Œåç»­ä¼šé€æ­¥æ‰©å±•æ›´å¤šæ¨¡å‹ä¸æ¨ç†æ–¹æ¡ˆã€‚

è¿™æ˜¯ä¸€ä¸ªæ”¾åœ¨ä»“åº“æ ¹ç›®å½•çš„ VoxCPM 1.5B ONNX CPU æ¨ç†é¡¹ç›®ï¼ŒåŸºäº `Text-to-Speech-TTS-ONNX/VoxCPM` æ”¹é€ ï¼Œæä¾›ï¼š

- ä¸€é”®å¯¼å‡º ONNX
- CPU é‡åŒ–ä¼˜åŒ–
- æ¨ç† CLIï¼ˆé»˜è®¤å¯ç”¨ text-normalizerï¼‰
- é¢„ç½®éŸ³è‰² + è¯­éŸ³å…‹éš†

## ç›®å½•ç»“æ„ (å»ºè®®)

```
.
â”œâ”€â”€ Export_VoxCPM_ONNX.py
â”œâ”€â”€ Optimize_ONNX.py
â”œâ”€â”€ infer.py
â”œâ”€â”€ config.json
â”œâ”€â”€ voices.json
â”œâ”€â”€ api_server.py                # FastAPI æœåŠ¡ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
â”œâ”€â”€ engines/                      # å¼•æ“æŠ½è±¡å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                   # æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ voxcpm_15b.py             # 1.5B å¼•æ“
â”‚   â””â”€â”€ voxcpm_05b.py             # 0.5B å¼•æ“ï¼ˆå®éªŒæ€§ï¼‰
â”œâ”€â”€ reference/                    # å®˜æ–¹ç¤ºä¾‹éŸ³è‰² (prompt audio)
â”œâ”€â”€ clone_reference/              # ç”¨æˆ·è‡ªå®šä¹‰å‚è€ƒéŸ³è‰²
â”œâ”€â”€ download_reference_voices.py
â”œâ”€â”€ run_service.sh
â”œâ”€â”€ modeling_modified/
â”œâ”€â”€ VoxCPM/                       # å®˜æ–¹æºç ï¼ˆç”¨äºå¯¼å‡ºï¼‰
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ VoxCPM1.5/                # å®˜æ–¹æƒé‡ + tokenizer/config
â”‚   â”œâ”€â”€ onnx_models/              # å¯¼å‡ºçš„å…¨ç²¾åº¦ ONNXï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
â”‚   â”œâ”€â”€ onnx_models_05b/          # 0.5B ONNXï¼ˆå®éªŒæ€§ï¼‰
â”‚   â””â”€â”€ onnx_models_quantized/    # CPU é‡åŒ–åçš„ ONNXï¼ˆå®éªŒæ€§ï¼‰
```


## ä¸€é”®å¯åŠ¨ (æ¨è)

```bash
./run_service.sh
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- ä¸‹è½½ VoxCPM1.5 æƒé‡
- å¦‚é…ç½®äº†é¢„æ„å»º ONNXï¼Œä¼šä¼˜å…ˆä¸‹è½½å¹¶ä½¿ç”¨ï¼ˆå¦åˆ™èµ°å¯¼å‡º+é‡åŒ–ï¼‰
- å¯¼å‡º ONNX
- CPU é‡åŒ–
- åŒæ­¥å®˜æ–¹ç¤ºä¾‹éŸ³è‰²
- ç”Ÿæˆä¸€æ®µä¸­è‹±æ–‡æ··åˆæµ‹è¯•éŸ³é¢‘åˆ° `outputs/demo.wav`

### ğŸ”¥ æ€§èƒ½ä¼˜åŒ–æç¤º

**é¢„çƒ­æ¨¡å‹** (å‡å°‘é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ)ï¼š
```python
# å¯åŠ¨æ—¶é¢„çƒ­
engine.synthesize(["warmup"], voice="default")  # ç¬¬ä¸€æ¬¡è°ƒç”¨æ…¢ï¼Œåç»­å¿«
```

**æµå¼æ’­æ”¾** (è¾¹ç”Ÿæˆè¾¹æ’­æ”¾ï¼Œé€‚åˆ LLM åœºæ™¯)ï¼š
```python
# LLM è¾“å‡ºæ—¶æŒ‰å¥æ‹†åˆ†
for sentence in split_sentences(llm_output):
    for chunk in tts.stream(sentence):
        play_audio(chunk)  # è¾¹ç”Ÿæˆè¾¹æ’­æ”¾
```

**è¿æ¥å¤ç”¨** (HTTP è¯·æ±‚)ï¼š
```python
session = requests.Session()  # å¤ç”¨ TCP è¿æ¥
session.post("http://localhost:8000/synthesize", json=payload)
```

## âš ï¸ é‡è¦ï¼šæ¨¡å‹ç‰ˆæœ¬é€‰æ‹©

æœ¬é¡¹ç›®æä¾›ä¸¤ç§ ONNX æ¨¡å‹ç‰ˆæœ¬ï¼š

### 1. å…¨ç²¾åº¦ç‰ˆæœ¬ (`onnx_models/`) - **æ¨è**
- âœ… è·¨å¹³å°å…¼å®¹æ€§å¥½ï¼ˆMac/Linux/Windowsï¼‰
- âœ… éŸ³è´¨ç¨³å®šå¯é 
- âš ï¸ ä½“ç§¯è¾ƒå¤§ï¼ˆ~6GBï¼‰
- âš ï¸ æ¨ç†é€Ÿåº¦ç¨æ…¢

### 2. é‡åŒ–ç‰ˆæœ¬ (`onnx_models_quantized/`) - **å®éªŒæ€§**
- âœ… ä½“ç§¯å°ï¼ˆ~1.5GBï¼‰
- âœ… æ¨ç†é€Ÿåº¦å¿«
- âŒ **åœ¨æŸäº› Linux å¹³å°ï¼ˆColab/GitHub Actionsï¼‰ä¸Šå¯èƒ½äº§ç”ŸéŸ³é¢‘å¤±çœŸ**
- âœ… Mac ä¸Šæµ‹è¯•æ­£å¸¸

**å»ºè®®ï¼š**
- **ç”Ÿäº§ç¯å¢ƒã€Colabã€GitHub Actions**: ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬
- **æœ¬åœ°å¼€å‘ï¼ˆMacï¼‰**: å¯å°è¯•é‡åŒ–ç‰ˆæœ¬
- å¦‚æœé‡åˆ°éŸ³é¢‘å¤±çœŸé—®é¢˜ï¼Œè¯·åˆ‡æ¢åˆ°å…¨ç²¾åº¦ç‰ˆæœ¬

## é¢„æ„å»º ONNX ä¸‹è½½ï¼ˆåŠ é€Ÿ CI/Colabï¼‰

å·²åœ¨ Hugging Face ä¸Šä¼ ä¸¤ä¸ªç‰ˆæœ¬ï¼š
- å…¨ç²¾åº¦ï¼š`Oulasong/voxcpm-onnx/onnx_models/` (main åˆ†æ”¯)
- é‡åŒ–ï¼š`Oulasong/voxcpm-onnx/onnx_models_quantized/` (quantized åˆ†æ”¯)

é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šä¸‹è½½ç‰ˆæœ¬ï¼š

```bash
# ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬ï¼ˆæ¨èç”¨äº Colab/GitHub Actionsï¼‰
VOXCPM_USE_QUANTIZED=0 \
VOXCPM_ONNX_REPO=Oulasong/voxcpm-onnx \
VOXCPM_ONNX_FORCE=1 \
./run_service.sh

# ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼ˆä»…æ¨è Mac æœ¬åœ°ä½¿ç”¨ï¼‰
VOXCPM_USE_QUANTIZED=1 \
VOXCPM_ONNX_REPO=Oulasong/voxcpm-onnx \
VOXCPM_ONNX_REVISION=quantized \
VOXCPM_ONNX_FORCE=1 \
./run_service.sh



è¯´æ˜ï¼š
- `VOXCPM_USE_QUANTIZED=0`: ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
- `VOXCPM_USE_QUANTIZED=1`: ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼ˆå¯èƒ½åœ¨æŸäº›å¹³å°å¤±çœŸï¼‰
- `VOXCPM_ONNX_FORCE=1`ï¼šå³ä½¿æœ¬åœ°å·²æœ‰ç¼“å­˜ä¹Ÿä¼šä¼˜å…ˆå°è¯•ä¸‹è½½ï¼›å¤±è´¥åˆ™ä¿ç•™å·²æœ‰æ–‡ä»¶ã€‚
- `VOXCPM_ONNX_REVISION`ï¼šæŒ‡å®š Hugging Face åˆ†æ”¯ï¼ˆmain=å…¨ç²¾åº¦ï¼Œquantized=é‡åŒ–ç‰ˆï¼‰
- `VOXCPM_MODEL_REPO`ï¼šæŒ‡å®šæƒé‡æ¥æºï¼ˆé»˜è®¤ openbmb/VoxCPM1.5ï¼‰



## 1) ä½¿ç”¨ uv ç®¡ç†ç¯å¢ƒ

å®‰è£… uvï¼š

```bash
pip install uv
```

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ï¼š

```bash
uv venv
uv pip install -r requirements-cpu.txt
```

å¯¼å‡ºä¸é‡åŒ–éœ€è¦é¢å¤–ä¾èµ–ï¼š

```bash
uv pip install -r requirements-export.txt
```

## 2) ä¸‹è½½å®˜æ–¹ç¤ºä¾‹éŸ³è‰² (reference)

```bash
uv run python download_reference_voices.py \
  --output-dir reference \
  --voices-file voices.json
```

å¦‚æœéœ€è¦é‡ç½®ä¸ºå®˜æ–¹ç¤ºä¾‹éŸ³è‰²åˆ—è¡¨ï¼ˆä¿ç•™ `default`ï¼‰ï¼ŒåŠ ä¸Š `--reset`ã€‚

## 3) ä¸‹è½½ VoxCPM1.5 æƒé‡

```python
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="openbmb/VoxCPM1.5",  # å¯ç”¨ VOXCPM_MODEL_REPO è¦†ç›–
    local_dir="./models/VoxCPM1.5",
    local_dir_use_symlinks=False,
)
```

## 4) å¯¼å‡º ONNX

```bash
python Export_VoxCPM_ONNX.py \
  --voxcpm-dir ./models/VoxCPM1.5 \
  --onnx-dir ./models/onnx_models
```

é»˜è®¤ä¸è¿è¡Œæ¨ç†æµ‹è¯•ï¼Œå¦‚éœ€å¯¼å‡ºåæµ‹è¯•ï¼š

```bash
python Export_VoxCPM_ONNX.py \
  --voxcpm-dir ./models/VoxCPM1.5 \
  --onnx-dir ./models/onnx_models \
  --run-infer
```

## 5) CPU é‡åŒ–ä¼˜åŒ–

```bash
python Optimize_ONNX.py \
  --input-dir ./models/onnx_models \
  --output-dir ./models/onnx_models_quantized \
  --cpu
```

## 6) æ¨ç†

**ç›´æ¥åˆæˆ** (é»˜è®¤è¯»å– `./models/onnx_models_quantized` å’Œ `./models/VoxCPM1.5`):

```bash
python infer.py \
  --text "ä½ å¥½ï¼Œæˆ‘æ˜¯ VoxCPM 1.5B çš„ ONNX ç‰ˆæœ¬ã€‚" \
  --output output.wav
```

**è¯­éŸ³å…‹éš†** (prompt audio + prompt text):

```bash
python infer.py \
  --text "è¿™æ˜¯ä¸€ä¸ªè¯­éŸ³å…‹éš†æµ‹è¯•ã€‚" \
  --prompt-audio ./clone_reference/prompt.wav \
  --prompt-text "å‚è€ƒè¯­éŸ³çš„æ–‡å­—å†…å®¹" \
  --output cloned.wav
```

**é¢„ç½®éŸ³è‰²**:

```bash
python infer.py --list-voices
python infer.py --voice default --text "ä½¿ç”¨é¢„ç½®éŸ³è‰²" --output preset.wav
```

ä¿®æ”¹ `voices.json` å¯ä»¥æ·»åŠ ä½ è‡ªå·±çš„éŸ³è‰²ã€‚è‡ªå®šä¹‰å‚è€ƒéŸ³è‰²å¯ä»¥æ”¾åˆ° `clone_reference/`ï¼Œç„¶åç”¨ `--prompt-audio/--prompt-text` è¿›è¡Œå…‹éš†ã€‚

## åœ¨å…¶ä»–é¡¹ç›®ä¸­è°ƒç”¨ï¼ˆCLIï¼‰

åœ¨ä½ çš„é¡¹ç›®é‡Œç”Ÿæˆä¸´æ—¶ config å¹¶ç›´æ¥è°ƒç”¨æœ¬ä»“åº“çš„ `infer.py`ï¼š

```python
import json
import subprocess

cfg = {
  "models_dir": "/path/ONNX_Lab/models/onnx_models",  # ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
  "voxcpm_dir": "/path/ONNX_Lab/models/VoxCPM1.5",
  "voices_file": "/path/your_project/voices.json",
  "voice": "default",
  "prompt_audio": None,
  "prompt_text": None,
  "text": "ä½ å¥½ï¼Œè¿™æ˜¯æµ‹è¯•ã€‚",
  "output": "out.wav"
}

with open("tmp_config.json", "w", encoding="utf-8") as f:
    json.dump(cfg, f, ensure_ascii=False, indent=2)

subprocess.run(["python", "/path/ONNX_Lab/infer.py", "--config", "tmp_config.json"], check=True)
```

è¯­éŸ³å…‹éš†æ—¶æŠŠ `voice` è®¾ä¸º `null`ï¼ŒåŒæ—¶æä¾› `prompt_audio` + `prompt_text` å³å¯ã€‚
`voices.json` ä¸­çš„ç›¸å¯¹è·¯å¾„ä¼šç›¸å¯¹å®ƒè‡ªèº«æ‰€åœ¨ç›®å½•è§£æã€‚

## API æœåŠ¡ï¼ˆFastAPIï¼‰

æ¨èï¼šä¸Šä¼ å‚è€ƒéŸ³é¢‘æ—¶ç”¨ **multipart**ï¼Œä»…ä½¿ç”¨é¢„ç½®éŸ³è‰²æ—¶ç”¨ **JSON**ã€‚

> âš¡ **æ–°å¢**ï¼šæ”¯æŒæµå¼è¾“å‡º (`/synthesize-stream`)ï¼Œå¯å®ç°ä½å»¶è¿Ÿå®æ—¶ TTS æ’­æ”¾ã€‚

å®‰è£…ï¼š

```bash
uv pip install -r requirements-api.txt
```

å¯åŠ¨ï¼š

```bash
uv run python -m uvicorn api_server:app --host 0.0.0.0 --port 8000
```

ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š

```bash
# ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
export VOXCPM_MODELS_DIR=/path/ONNX_Lab/models/onnx_models
export VOXCPM_VOXCPM_DIR=/path/ONNX_Lab/models/VoxCPM1.5
export VOXCPM_VOICES_FILE=/path/your_project/voices.json
export VOXCPM_MAX_CONCURRENCY=1

# æˆ–ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼ˆä»… Macï¼Œå¯èƒ½åœ¨ Linux å¤±çœŸï¼‰
# export VOXCPM_MODELS_DIR=/path/ONNX_Lab/models/onnx_models_quantized
```

å¤šç”¨æˆ·å»ºè®®ï¼šæé«˜ `VOXCPM_MAX_CONCURRENCY` æˆ–ä½¿ç”¨ `uvicorn --workers N`ï¼ˆæ¯ä¸ª worker ä¼šåŠ è½½ä¸€ä»½æ¨¡å‹ï¼Œå ç”¨æ›´å¤šå†…å­˜ï¼‰ã€‚

### API ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/info` | GET | è·å–éŸ³é¢‘å‚æ•° (sample_rate, bit_depth, channels) |
| `/voices` | GET | åˆ—å‡ºå¯ç”¨éŸ³è‰² |
| `/synthesize` | POST | åˆæˆè¯­éŸ³ï¼Œè¿”å› WAV æ–‡ä»¶ |
| `/synthesize-file` | POST | æ”¯æŒä¸Šä¼ å‚è€ƒéŸ³é¢‘çš„åˆæˆ |
| `/synthesize-stream` | POST | **æµå¼è¾“å‡º** Raw PCM (int16)ï¼Œé€‚åˆå®æ—¶æ’­æ”¾ |

### JSON è¯·æ±‚ç¤ºä¾‹ï¼ˆé¢„ç½®éŸ³è‰²ï¼‰

```bash
curl -X POST http://localhost:8000/synthesize \
  -H "Content-Type: application/json" \
  -d '{"text":"ä½ å¥½ï¼Œè¿™æ˜¯æµ‹è¯•ã€‚","voice":"default"}' \
  --output out.wav
```

### æµå¼è¾“å‡ºç¤ºä¾‹

```bash
# è·å–éŸ³é¢‘å‚æ•°
curl -s http://localhost:8000/info
# è¿”å›: {"sample_rate": 44100, "bit_depth": 16, "channels": 1}

# æµå¼åˆæˆï¼ˆè¿”å› Raw PCMï¼‰
curl -X POST http://localhost:8000/synthesize-stream \
  -H "Content-Type: application/json" \
  -d '{"text":"è¿™æ˜¯æµå¼è¾“å‡ºæµ‹è¯•"}' \
  --output stream.pcm

# æ’­æ”¾ PCM (Mac/Linux)
ffplay -f s16le -ar 44100 -ac 1 stream.pcm
```

å“åº”å¤´åŒ…å«éŸ³é¢‘æ ¼å¼ä¿¡æ¯ï¼š
- `X-Sample-Rate`: é‡‡æ ·ç‡ (44100)
- `X-Bit-Depth`: ä½æ·± (16)
- `X-Channels`: å£°é“æ•° (1)

### multipart è¯·æ±‚ç¤ºä¾‹ï¼ˆè¯­éŸ³å…‹éš†ï¼‰

```bash
curl -X POST http://localhost:8000/synthesize-file \
  -F "text=è¿™æ˜¯è¯­éŸ³å…‹éš†æµ‹è¯•ã€‚" \
  -F "prompt_text=å‚è€ƒè¯­éŸ³çš„æ–‡å­—å†…å®¹" \
  -F "prompt_audio=@/path/to/prompt.wav" \
  --output cloned.wav
```

### Python å®¢æˆ·ç«¯ç¤ºä¾‹ï¼ˆæµå¼æ’­æ”¾ï¼‰

```python
import requests
import pyaudio

# è·å–éŸ³é¢‘å‚æ•°
info = requests.get("http://localhost:8000/info").json()
print(f"Sample rate: {info['sample_rate']}Hz")

# åˆå§‹åŒ– PyAudio
p = pyaudio.PyAudio()
stream = p.open(
    format=pyaudio.paInt16,
    channels=info['channels'],
    rate=info['sample_rate'],
    output=True
)

# æµå¼æ’­æ”¾
with requests.post(
    "http://localhost:8000/synthesize-stream",
    json={"text": "å®æ—¶æµå¼æ’­æ”¾æµ‹è¯•"},
    stream=True
) as r:
    for chunk in r.iter_content(chunk_size=4096):
        stream.write(chunk)

stream.stop_stream()
stream.close()
p.terminate()
```

### é«˜çº§åŠŸèƒ½

#### é‡è¯•æœºåˆ¶ (`retry_badcase`)

è‡ªåŠ¨æ£€æµ‹å¹¶é‡è¯•ç”Ÿæˆå¼‚å¸¸çš„ "unstoppable" æƒ…å†µï¼š

```bash
curl -X POST http://localhost:8000/synthesize \
  -H "Content-Type: application/json" \
  -d '{
    "text": "ä½ å¥½ä¸–ç•Œ",
    "voice": "default",
    "retry_badcase": true,
    "max_retries": 3,
    "length_ratio_threshold": 6.0
  }' \
  --output out.wav
```

#### Prompt Cacheï¼ˆå¤šå¥è¿ç»­ç”Ÿæˆï¼‰

æ”¯æŒ LLM æµå¼åœºæ™¯ï¼Œä¿æŒå¤šå¥ç”Ÿæˆçš„å£°éŸ³ä¸€è‡´æ€§ï¼š

```python
from engines.voxcpm_15b import VoxCPM15BEngine

engine = VoxCPM15BEngine(...)

# æ„å»º prompt cache
cache = engine.build_prompt_cache("prompt.wav", "å‚è€ƒæ–‡æœ¬")

# å¤šå¥æµå¼ç”Ÿæˆ
sentences = ["ç¬¬ä¸€å¥ã€‚", "ç¬¬äºŒå¥ã€‚", "ç¬¬ä¸‰å¥ã€‚"]
for sentence in sentences:
    audio, sr = engine.synthesize([sentence], prompt_audio="prompt.wav", prompt_text="å‚è€ƒæ–‡æœ¬")
    # æ’­æ”¾æˆ–ä¿å­˜ audio
    # å¯é€‰ï¼šåˆå¹¶ cache ä¿æŒé•¿æœŸä¸€è‡´æ€§
    # cache = engine.merge_prompt_cache(cache, generated_feats)
```


## é…ç½®æ–‡ä»¶ (config.json)

`config.json` æ˜¯è¿è¡Œæ—¶é…ç½®ï¼ˆä¸æ˜¯ ONNX é…ç½®ï¼‰ï¼Œå¿…é¡»æ˜¯ **çº¯ JSON**ã€‚

### å­—æ®µè¯´æ˜

- `models_dir`: ONNX æ¨¡å‹ç›®å½•è·¯å¾„ã€‚âš ï¸ **å¹³å°å…¼å®¹æ€§è¯´æ˜**ï¼š
  - **Colab/GitHub Actions/Linux**: æ¨èä½¿ç”¨ `models/onnx_models`ï¼ˆå…¨ç²¾åº¦ç‰ˆæœ¬ï¼‰
  - **Mac æœ¬åœ°å¼€å‘**: å¯ä½¿ç”¨ `models/onnx_models_quantized`ï¼ˆé‡åŒ–ç‰ˆæœ¬ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰
  - é‡åŒ–ç‰ˆæœ¬åœ¨æŸäº› Linux å¹³å°å¯èƒ½äº§ç”ŸéŸ³é¢‘å¤±çœŸï¼Œé‡åˆ°é—®é¢˜è¯·åˆ‡æ¢åˆ°å…¨ç²¾åº¦ç‰ˆæœ¬
  - è¿™é‡Œçš„ ONNX æ¥è‡ª `openbmb/VoxCPM1.5` æƒé‡å¯¼å‡ºï¼Œ**å¹¶éå®˜æ–¹ç›´æ¥æä¾›çš„ ONNX**
  - è¯¥ç›®å½•åŒ…å« `voxcpm_onnx_config.json`ï¼ˆé‡‡æ ·ç‡ã€æ­¥æ•°ç­‰é»˜è®¤å€¼ï¼‰
- `voxcpm_dir`: `openbmb/VoxCPM1.5` æƒé‡ç›®å½•ï¼ˆç”¨äº tokenizer/configï¼‰ã€‚
- `voice`: é¢„ç½®éŸ³è‰²åç§°ï¼ˆæ¥è‡ª `voices.json`ï¼‰ã€‚ä½¿ç”¨ `voice` æ—¶è¯·ä¿æŒ `prompt_audio`/`prompt_text` ä¸º `null`ã€‚å¦‚æœä½¿ç”¨`prompt_audio`,åˆ™`voice`å¡«null
- `prompt_audio`: è¯­éŸ³å…‹éš†å‚è€ƒéŸ³é¢‘è·¯å¾„ï¼ˆä¸ `prompt_text` æˆå¯¹å‡ºç°ï¼‰ã€‚
  æ¨è `wav`/`flac`/`ogg`ï¼Œè‡ªåŠ¨è½¬å•å£°é“ã€é‡é‡‡æ ·è‡³ 44.1kã€‚
  é»˜è®¤æœ€å¤§ 20 ç§’ï¼ˆè¶…å‡ºä¼šæˆªæ–­ï¼‰ï¼Œæ¨è 3~15 ç§’å¹²å‡€è¯­éŸ³ã€‚
- `prompt_text`: å‚è€ƒéŸ³é¢‘çš„æ–‡å­—å†…å®¹ï¼ˆå¿…é¡»ä¸ `prompt_audio` å¯¹åº”ï¼‰ã€‚
- `text`: å¾…åˆæˆå†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–æ•°ç»„ï¼‰ã€‚æ•°ç»„è¡¨ç¤ºå¤šå¥è¾“å…¥ï¼Œä¼šåœ¨å¥å­é—´æ’å…¥çŸ­åœé¡¿ã€‚
  é•¿æ–‡æœ¬å¯ä»¥ç›´æ¥å†™æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½†æ¨èæ‹†å¥æˆ–ä½¿ç”¨ `text_file`ã€‚
- `text_file`: æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€å¥ï¼‰ã€‚
- `output`: è¾“å‡º WAV è·¯å¾„ã€‚
- `text_normalizer`: æ–‡æœ¬å½’ä¸€åŒ–å¼€å…³ï¼ˆé»˜è®¤ `true`ï¼‰ï¼Œä¼šå¤„ç†æ•°å­—/è‹±æ–‡/æ ‡ç‚¹ç­‰ã€‚
- `audio_normalizer`: éŸ³é¢‘ RMS å½’ä¸€åŒ–å¼€å…³ï¼ˆé»˜è®¤ `false`ï¼‰ï¼Œç”¨äºå‚è€ƒéŸ³é¢‘å’Œè¾“å‡ºéŸ³é¢‘çš„éŸ³é‡æ ¡å‡†ã€‚
- `cfg_value`: CFG å¼•å¯¼å°ºåº¦ï¼ˆè¶Šé«˜è¶Šè´´è¿‘æç¤ºè¯­ï¼Œä½†è¿‡é«˜å¯èƒ½å‘é—·/å¤±çœŸï¼‰ã€‚å¸¸ç”¨ 2.0~3.0ã€‚
- `fixed_timesteps`: è§£ç æ­¥æ•°ï¼ˆè¶Šé«˜è¶Šæ…¢ï¼Œå¯èƒ½æ›´ç¨³ï¼‰ã€‚å¸¸ç”¨ 8~12ã€‚
- `seed`: éšæœºç§å­ï¼ˆç”¨äºå¯å¤ç°æ€§ï¼‰ã€‚
- `max_threads`: ONNXRuntime CPU çº¿ç¨‹æ•°ï¼ˆ`0` = è‡ªåŠ¨ï¼›GitHub Actions å¯è®¾ 2ï¼‰ã€‚
- `model_size`: æ¨¡å‹å¤§å°é€‰æ‹©ï¼ˆ`"1.5b"` æˆ– `"0.5b"`ï¼‰ã€‚âš ï¸ **0.5B ç›®å‰ä¸ºå®éªŒæ€§åŠŸèƒ½**ã€‚

> è¯­è¨€æ”¯æŒï¼šæ¨¡å‹ä»¥ä¸­æ–‡/è‹±æ–‡ä¸ºä¸»ï¼Œå…¶ä»–è¯­è¨€ä¸ä¿è¯æ•ˆæœã€‚

### ç¤ºä¾‹ï¼šé¢„ç½®éŸ³è‰²

```json
{
  "voice": "default",
  "prompt_audio": null,
  "prompt_text": null
}
```

### ç¤ºä¾‹ï¼šè‡ªå®šä¹‰è¯­éŸ³å…‹éš†

```json
{
  "voice": null,
  "prompt_audio": "clone_reference/my.wav",
  "prompt_text": "è¿™æ˜¯å‚è€ƒéŸ³é¢‘çš„æ–‡å­—å†…å®¹ã€‚"
}
```

é…ç½®ä¼˜å…ˆçº§ï¼š**å‘½ä»¤è¡Œå‚æ•° > config.json > é»˜è®¤å€¼**ã€‚

## å®˜æ–¹éŸ³è‰²è¯´æ˜

`download_reference_voices.py` ä¼šä»å®˜æ–¹ demo page ä¸‹è½½ prompt éŸ³é¢‘åˆ° `reference/`ï¼Œ
å¹¶è‡ªåŠ¨å†™å…¥ `voices.json`ï¼ˆä¾› `--voice` ä½¿ç”¨ï¼‰ã€‚`reference/basic_ref_zh.wav` ä¸ºé»˜è®¤éŸ³è‰²ç¤ºä¾‹ã€‚
åŒæ—¶ä¼šä¸‹è½½å®˜æ–¹ **Context-Aware Speech Generation** ç¤ºä¾‹éŸ³é¢‘å¹¶å†™å…¥ `voices.json`
ï¼ˆä¾‹å¦‚ `context_zh_story_telling`ï¼‰ã€‚

## Context-Aware Speech Generation (w/o prompt speech)

è¯¥æ¨¡å¼ **ä¸ä½¿ç”¨ prompt audio**ï¼Œä»…ä¾èµ–æ–‡æœ¬æœ¬èº«çš„è¯­ä¹‰è®©æ¨¡å‹è‡ªåŠ¨æ¨æ–­è¯­æ°”ä¸é£æ ¼ã€‚
ç¤ºä¾‹æ–‡æœ¬å·²æ•´ç†åœ¨ `context_aware_texts.json`ï¼Œå…¶ä¸­ `type` åªæ˜¯æè¿°æ ‡ç­¾ **ä¸ä½œä¸ºè¾“å…¥**ã€‚

```json
{
  "voice": null,
  "prompt_audio": null,
  "prompt_text": null,
  "text": ["åœ¨å¾ˆä¹…å¾ˆä¹…ä»¥å‰ï¼Œæœ‰ä¸€ä¸ªå›½ç‹ã€‚"]
}
```

ç›´æ¥å‘½ä»¤è¡Œä¹Ÿå¯ï¼š

```bash
python infer.py --text "åœ¨å¾ˆä¹…å¾ˆä¹…ä»¥å‰ï¼Œæœ‰ä¸€ä¸ªå›½ç‹ã€‚" --output out.wav
```

## Normalizer

é»˜è®¤å¼€å¯ text-normalizerï¼›å¦‚éœ€å…³é—­ï¼š

```bash
python infer.py --no-text-normalizer --text "..." --output out.wav
```

æˆ–åœ¨ `config.json` ä¸­è®¾ç½® `text_normalizer: false`ã€‚

éŸ³é¢‘å½’ä¸€åŒ–é»˜è®¤å…³é—­ï¼›å¦‚éœ€å¼€å¯ï¼š

```bash
python infer.py --audio-normalizer --text "..." --output out.wav
```

æˆ–åœ¨ `config.json` ä¸­è®¾ç½® `audio_normalizer: true`ã€‚

## Colab å¿«é€Ÿä½“éªŒ

âš ï¸ **Colab å¿…é¡»ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬ï¼Œé‡åŒ–ç‰ˆæœ¬ä¼šå¯¼è‡´éŸ³é¢‘å¤±çœŸï¼**

```python
# 1. å…‹éš†ä»“åº“å¹¶å®‰è£… uv
!git clone https://github.com/realAllenSong/ONNX_Lab.git
%cd ONNX_Lab
!pip install uv
!chmod +x run_service.sh

# 2. ä¸‹è½½å…¨ç²¾åº¦ ONNX æ¨¡å‹å¹¶ç”ŸæˆéŸ³é¢‘ï¼ˆæ¨èï¼‰
import os
os.environ['VOXCPM_USE_QUANTIZED'] = '0'
os.environ['VOXCPM_ONNX_REPO'] = 'Oulasong/voxcpm-onnx'
os.environ['VOXCPM_ONNX_FORCE'] = '1'
!./run_service.sh

# 3. æ’­æ”¾è¾“å‡ºéŸ³é¢‘
from IPython.display import Audio
Audio("outputs/demo.wav")
```

## GitHub Actionsï¼ˆä¸‹è½½ä¼˜å…ˆ + cache fallbackï¼‰

âš ï¸ **GitHub Actions å¿…é¡»ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬ï¼Œé‡åŒ–ç‰ˆæœ¬ä¼šå¯¼è‡´éŸ³é¢‘å¤±çœŸï¼**

æ¨èæµç¨‹ï¼šå…ˆå°è¯•ä¸‹è½½é¢„æ„å»ºå…¨ç²¾åº¦ ONNXï¼Œå¤±è´¥åˆ™ä½¿ç”¨ cacheï¼ˆæˆ–èµ°æœ¬åœ°å¯¼å‡ºï¼‰ã€‚
ç¤ºä¾‹ workflowï¼š`.github/workflows/voxcpm_cpu.yml`

```yaml
name: voxcpm-cpu
on:
  workflow_dispatch:

jobs:
  infer:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - name: Install uv
        run: pip install uv
      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: |
            models/VoxCPM1.5
            models/onnx_models
          key: voxcpm-onnx-full-${{ runner.os }}-v2
      - name: Run
        env:
          VOXCPM_USE_QUANTIZED: "0"  # ä½¿ç”¨å…¨ç²¾åº¦ç‰ˆæœ¬
          VOXCPM_ONNX_REPO: Oulasong/voxcpm-onnx
          VOXCPM_ONNX_FORCE: "1"
        run: |
          chmod +x run_service.sh
          ./run_service.sh
      - name: Upload output
        uses: actions/upload-artifact@v4
        with:
          name: demo-audio
          path: outputs/demo.wav
```

## Credits

- Text-to-Speech-TTS-ONNX: https://github.com/DakeQQ/Text-to-Speech-TTS-ONNX
- VoxCPM-ONNX: https://github.com/bluryar/VoxCPM-ONNX
- VoxCPM (official): https://github.com/OpenBMB/VoxCPM
